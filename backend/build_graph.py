#!/usr/bin/env python3
"""
ä½¿ç”¨æµæ°´çº¿ä¸­çš„Neo4jUploaderæ„å»ºèŒä½çŸ¥è¯†å›¾è°±
"""

import os
import sys
import importlib.util
import pandas as pd
from neo4j import GraphDatabase

# é…ç½®Neo4jè¿æ¥ä¿¡æ¯
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "TYH041113"

# æ¸…æ´—åçš„æ•°æ®è·¯å¾„
CLEAN_DATA_PATH = "../æ¨¡å—_æ•°æ®å¤„ç†/æ¸…æ´—è¾“å‡º/ç¬¬äºŒæ¬¡æ¸…æ´—/"

class Neo4jUploader:
    """Neo4jçŸ¥è¯†å›¾è°±ä¸Šä¼ å™¨"""

    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        print(f"âœ… å·²è¿æ¥åˆ°Neo4j: {uri}")
        
        # æ˜¾ç¤ºå½“å‰æ•°æ®åº“çŠ¶æ€
        try:
            total_nodes, total_rels = self.get_neo4j_stats()
            print(f"ğŸ“Š å½“å‰æ•°æ®åº“: {total_nodes:,} ä¸ªèŠ‚ç‚¹, {total_rels:,} æ¡å…³ç³»")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–æ•°æ®åº“ç»Ÿè®¡: {e}")

    def close(self):
        self.driver.close()

    def get_neo4j_stats(self):
        """è·å–Neo4jæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        with self.driver.session() as session:
            result = session.run("CALL db.stats.retrieve('GRAPH COUNTS')")
            stats = result.single()[0]
            total_nodes = stats['nodes']['total'] if 'nodes' in stats else 0
            total_rels = stats['relationships']['total'] if 'relationships' in stats else 0
            return total_nodes, total_rels

    def _is_valid_value(self, value):
        """æ£€æŸ¥å€¼æ˜¯å¦æœ‰æ•ˆ"""
        if value is None:
            return False
        if pd.isna(value):
            return False
        if isinstance(value, str) and value.strip() == '':
            return False
        return True

    def clear_database(self):
        """æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ•°æ®ï¼ˆå·²ç¦ç”¨ï¼‰"""
        print("âš ï¸  æ¸…ç©ºæ•°æ®åº“åŠŸèƒ½å·²ç¦ç”¨ï¼Œä¿æŠ¤ç°æœ‰æ•°æ®å®‰å…¨")
        return False

    def create_job_graph(self, job_data):
        """åˆ›å»ºèŒä½å›¾è°±"""
        import re
        
        with self.driver.session() as session:
            city = job_data.get('åŸå¸‚')
            company = job_data.get('å…¬å¸')
            industry = job_data.get('è¡Œä¸š')
            scale = job_data.get('å…¬å¸è§„æ¨¡', '')
            
            has_city = self._is_valid_value(city)
            has_company = self._is_valid_value(company)
            has_industry = self._is_valid_value(industry)
            
            # åˆ›å»ºåŸå¸‚
            if has_city:
                session.run("MERGE (city:City {name: $name})", name=city)
            
            # åˆ›å»ºå…¬å¸
            if has_company:
                session.run("""
                    MERGE (c:Company {name: $name})
                    SET c.scale = $scale
                """, name=company, scale=scale if self._is_valid_value(scale) else '')
                
                if has_city:
                    session.run("""
                        MATCH (c:Company {name: $company})
                        MATCH (city:City {name: $city})
                        MERGE (c)-[:LOCATED_IN]->(city)
                    """, company=company, city=city)
            
            # åˆ›å»ºè¡Œä¸š
            if has_industry:
                session.run("MERGE (i:Industry {name: $name})", name=industry)
            
            # åˆ›å»ºèŒä½
            job_url = job_data.get('èŒä½URL', f"job_{hash(str(job_data))}")
            session.run("""
                MERGE (j:Job {url: $url})
                SET j.title = $title,
                    j.experience = $experience,
                    j.education = $education,
                    j.description = $description,
                    j.salary = $salary,
                    j.salary_min = $salary_min,
                    j.salary_max = $salary_max,
                    j.annual_months = $annual_months,
                    j.publish_time = $publish_time
            """,
                url=job_url,
                title=job_data.get('èŒä½', ''),
                experience=job_data.get('ç»éªŒ', ''),
                education=job_data.get('å­¦å†', ''),
                description=str(job_data.get('å·¥ä½œæè¿°', ''))[:500],
                salary=job_data.get('è–ªèµ„', ''),
                salary_min=job_data.get('è–ªèµ„_æœ€å°å€¼'),
                salary_max=job_data.get('è–ªèµ„_æœ€å¤§å€¼'),
                annual_months=job_data.get('å¹´è–ªæœˆæ•°'),
                publish_time=job_data.get('å‘å¸ƒæ—¶é—´', '')
            )
            
            # åˆ›å»ºå…³ç³»
            if has_company:
                session.run("""
                    MATCH (j:Job {url: $url})
                    MATCH (c:Company {name: $company})
                    MERGE (j)-[:OFFERED_BY]->(c)
                """, url=job_url, company=company)
            
            if has_industry:
                session.run("""
                    MATCH (j:Job {url: $url})
                    MATCH (i:Industry {name: $industry})
                    MERGE (j)-[:BELONGS_TO_INDUSTRY]->(i)
                """, url=job_url, industry=industry)
            
            # åˆ›å»ºæŠ€èƒ½å…³ç³»
            skills = job_data.get('æŠ€èƒ½', '')
            if skills and isinstance(skills, str):
                skill_list = re.split(r'[,ï¼Œã€;ï¼›/\\|]', skills)
                for skill in skill_list:
                    skill = skill.strip()
                    if skill and len(skill) >= 2:
                        session.run("MERGE (s:Skill {name: $name})", name=skill)
                        session.run("""
                            MATCH (j:Job {url: $url})
                            MATCH (s:Skill {name: $skill})
                            MERGE (j)-[:REQUIRES_SKILL]->(s)
                        """, url=job_url, skill=skill)

    def upload_cleaned_jobs(self, input_dir):
        """ä¸Šä¼ æ¸…æ´—åçš„èŒä½æ•°æ®"""
        print(f"ğŸ“ å¼€å§‹ä¸Šä¼ æ•°æ®: {input_dir}")
        
        # éå†æ‰€æœ‰å­ç›®å½•
        total_count = 0
        for subdir in os.listdir(input_dir):
            subdir_path = os.path.join(input_dir, subdir)
            if os.path.isdir(subdir_path):
                print(f"ğŸ” å¤„ç†ç›®å½•: {subdir}")
                
                # æ‰¾åˆ°æ‰€æœ‰CSVæ–‡ä»¶
                csv_files = [f for f in os.listdir(subdir_path) if f.endswith('_advanced.csv')]
                
                for csv_file in csv_files:
                    file_path = os.path.join(subdir_path, csv_file)
                    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {csv_file}")
                    
                    try:
                        df = pd.read_csv(file_path, encoding='utf-8')
                        file_count = 0
                        
                        for _, row in df.iterrows():
                            job_data = row.to_dict()
                            self.create_job_graph(job_data)
                            file_count += 1
                            total_count += 1
                            
                            if total_count % 100 == 0:
                                print(f"  ğŸ“Š æ€»è¿›åº¦: {total_count}")
                        
                        print(f"  âœ… å®Œæˆæ–‡ä»¶: {csv_file}, å¤„ç†äº† {file_count} æ¡è®°å½•")
                    
                    except Exception as e:
                        print(f"  âŒ å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
                        continue
        
        print(f"âœ… æ‰€æœ‰æ•°æ®ä¸Šä¼ å®Œæˆ: {total_count} æ¡è®°å½•")
        return total_count

def main():
    print("="*60)
    print("ğŸ—ï¸  æ„å»ºèŒä½çŸ¥è¯†å›¾è°±")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–ä¸Šä¼ å™¨
        uploader = Neo4jUploader(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)
        
        try:
            # æ¸…ç©ºæ•°æ®åº“
            if not uploader.clear_database():
                print("âŒ æ¸…ç©ºæ•°æ®åº“å¤±è´¥")
                return
            
            # ä¸Šä¼ æ•°æ®
            if os.path.exists(CLEAN_DATA_PATH):
                uploader.upload_cleaned_jobs(CLEAN_DATA_PATH)
            else:
                print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {CLEAN_DATA_PATH}")
                return
            
            # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
            total_nodes, total_rels = uploader.get_neo4j_stats()
            print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®åº“çŠ¶æ€: {total_nodes:,} ä¸ªèŠ‚ç‚¹, {total_rels:,} æ¡å…³ç³»")
            print("âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
            
        finally:
            uploader.close()
            
    except Exception as e:
        print(f"âŒ æ„å»ºå›¾è°±æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
