#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èŒä¸šæ•°æ®å¤„ç†æµæ°´çº¿
==================
ç»Ÿä¸€å¤„ç†èŒä¸šæ•°æ®ä»åŸå§‹CSVåˆ°Neo4jçŸ¥è¯†å›¾è°±çš„å®Œæ•´æµç¨‹

åŠŸèƒ½:
1. ç¬¬ä¸€æ¬¡æ•°æ®æ¸…æ´— - å»é™¤ç¼ºå¤±å¿…å¡«å­—æ®µçš„è®°å½•
2. ç¬¬äºŒæ¬¡æ•°æ®æ¸…æ´— - URLå»é‡ã€è–ªèµ„æ ‡å‡†åŒ–ã€æ•°æ®å¢å¼º
3. ä¸Šä¼ Neo4j - æ„å»ºçŸ¥è¯†å›¾è°±

ä½œè€…: AI Assistant
æ—¥æœŸ: 2026-01-12
"""

import os
import re
import json
import argparse
import pandas as pd
from datetime import datetime
from neo4j import GraphDatabase

# ==================== é…ç½®åŠ è½½ ====================
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = os.path.join(os.path.dirname(__file__), 'pipeline_config.json')
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)

CONFIG = load_config()

# ==================== å®‰å…¨æ£€æŸ¥å‡½æ•° ====================
def confirm_neo4j_operation():
    """ç¡®è®¤Neo4jå†™å…¥æ“ä½œ"""
    safety_config = CONFIG.get('safety', {})
    
    if not safety_config.get('require_confirmation', True):
        return True
    
    if safety_config.get('backup_reminder', True):
        print("\n" + "="*60)
        print("âš ï¸  å®‰å…¨æé†’")
        print("="*60)
        print("å³å°†å‘Neo4jæ•°æ®åº“å†™å…¥æ•°æ®ï¼")
        print("\nå»ºè®®æ“ä½œ:")
        print("  1. ç¡®è®¤å·²å¤‡ä»½Neo4jæ•°æ®åº“")
        print("  2. æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦æ­£ç¡®")
        print("  3. å»ºè®®å…ˆç”¨å°é‡æ•°æ®æµ‹è¯•")
        print("\næ•°æ®åº“è¿æ¥ä¿¡æ¯:")
        print(f"  URI: {CONFIG['neo4j']['uri']}")
        print(f"  ç”¨æˆ·: {CONFIG['neo4j']['user']}")
        print("="*60)
    
    while True:
        response = input("\næ˜¯å¦ç»§ç»­ä¸Šä¼ åˆ°Neo4j? (yes/no): ").strip().lower()
        if response in ['yes', 'y']:
            return True
        elif response in ['no', 'n']:
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return False
        else:
            print("âš ï¸  è¯·è¾“å…¥ yes æˆ– no")

def get_neo4j_stats(driver):
    """è·å–Neo4jæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    with driver.session() as session:
        result = session.run("MATCH (n) RETURN count(n) as total_nodes")
        total_nodes = result.single()['total_nodes']
        
        result = session.run("MATCH ()-[r]->() RETURN count(r) as total_rels")
        total_rels = result.single()['total_rels']
        
    return total_nodes, total_rels

# ==================== ç¬¬ä¸€æ¬¡æ¸…æ´— ====================
class FirstCleaner:
    """ç¬¬ä¸€æ¬¡æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self, required_fields):
        self.required_fields = required_fields
    
    def check_required_fields(self, row):
        """æ£€æŸ¥å¿…å¡«å­—æ®µ"""
        missing_fields = []
        for field in self.required_fields:
            value = row.get(field, None)
            if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):
                missing_fields.append(field)
        
        if missing_fields:
            return False, f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {', '.join(missing_fields)}"
        return True, ""
    
    def clean_city(self, input_dir, output_dir):
        """æ¸…æ´—æŒ‡å®šåŸå¸‚çš„æ•°æ®"""
        os.makedirs(output_dir, exist_ok=True)
        
        csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
        total_stats = {'original': 0, 'cleaned': 0, 'removed': 0}
        
        print(f"ğŸ“ ç¬¬ä¸€æ¬¡æ¸…æ´—: {input_dir}")
        
        for csv_file in csv_files:
            input_path = os.path.join(input_dir, csv_file)
            output_path = os.path.join(output_dir, csv_file)
            
            df = pd.read_csv(input_path, encoding='utf-8')
            total_stats['original'] += len(df)
            
            valid_rows = []
            for _, row in df.iterrows():
                is_valid, _ = self.check_required_fields(row)
                if is_valid:
                    valid_rows.append(row)
            
            if valid_rows:
                cleaned_df = pd.DataFrame(valid_rows)
                cleaned_df.to_csv(output_path, index=False, encoding='utf-8')
                total_stats['cleaned'] += len(cleaned_df)
                total_stats['removed'] += len(df) - len(cleaned_df)
        
        print(f"  âœ… åŸå§‹: {total_stats['original']}, æ¸…æ´—å: {total_stats['cleaned']}, åˆ é™¤: {total_stats['removed']}")
        return total_stats

# ==================== ç¬¬äºŒæ¬¡æ¸…æ´— ====================
class SecondCleaner:
    """ç¬¬äºŒæ¬¡æ•°æ®æ¸…æ´—å™¨ï¼ˆè¿›é˜¶ï¼‰"""
    
    @staticmethod
    def standardize_salary(salary_str):
        """æ ‡å‡†åŒ–è–ªèµ„"""
        if pd.isna(salary_str):
            return None
        
        salary_str = str(salary_str).strip()
        if salary_str in ['é¢è®®', '', 'nan']:
            return None
        
        # åŒ¹é… "4-7ä¸‡" æ ¼å¼
        match = re.search(r'(\d+\.?\d*)\s*-\s*(\d+\.?\d*)\s*ä¸‡', salary_str)
        if match:
            min_sal = float(match.group(1)) * 10000
            max_sal = float(match.group(2)) * 10000
            return f"{int(min_sal)}-{int(max_sal)}"
        
        # åŒ¹é… "4-7åƒ" æ ¼å¼
        match = re.search(r'(\d+\.?\d*)\s*-\s*(\d+\.?\d*)\s*åƒ', salary_str)
        if match:
            min_sal = float(match.group(1)) * 1000
            max_sal = float(match.group(2)) * 1000
            return f"{int(min_sal)}-{int(max_sal)}"
        
        return None
    
    @staticmethod
    def extract_salary_details(salary_str):
        """æå–è–ªèµ„è¯¦æƒ…"""
        if pd.isna(salary_str):
            return None, None, None
        
        salary_str = str(salary_str).strip()
        
        # æå–è–ªèµ„èŒƒå›´
        match = re.search(r'(\d+)-(\d+)', salary_str)
        if match:
            min_sal = int(match.group(1))
            max_sal = int(match.group(2))
        else:
            return None, None, None
        
        # æå–å¹´è–ªæœˆæ•°
        months_match = re.search(r'(\d+)è–ª', salary_str)
        annual_months = int(months_match.group(1)) if months_match else 12
        
        return min_sal, max_sal, annual_months
    
    def clean_city(self, input_dir, output_dir):
        """æ¸…æ´—æŒ‡å®šåŸå¸‚çš„æ•°æ®"""
        os.makedirs(output_dir, exist_ok=True)
        
        csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
        total_stats = {'original': 0, 'cleaned': 0, 'duplicates': 0}
        
        print(f"ğŸ“ ç¬¬äºŒæ¬¡æ¸…æ´—: {input_dir}")
        
        for csv_file in csv_files:
            input_path = os.path.join(input_dir, csv_file)
            output_file = csv_file.replace('.csv', '_advanced.csv')
            output_path = os.path.join(output_dir, output_file)
            
            df = pd.read_csv(input_path, encoding='utf-8')
            total_stats['original'] += len(df)
            
            # URLå»é‡
            df_cleaned = df.drop_duplicates(subset=['èŒä½URL'], keep='first')
            total_stats['duplicates'] += len(df) - len(df_cleaned)
            
            # è–ªèµ„å¤„ç†
            df_cleaned['è–ªèµ„_æœ€å°å€¼'] = None
            df_cleaned['è–ªèµ„_æœ€å¤§å€¼'] = None
            df_cleaned['å¹´è–ªæœˆæ•°'] = None
            
            for idx, row in df_cleaned.iterrows():
                min_sal, max_sal, months = self.extract_salary_details(row.get('è–ªèµ„'))
                df_cleaned.at[idx, 'è–ªèµ„_æœ€å°å€¼'] = min_sal
                df_cleaned.at[idx, 'è–ªèµ„_æœ€å¤§å€¼'] = max_sal
                df_cleaned.at[idx, 'å¹´è–ªæœˆæ•°'] = months
                
                standardized = self.standardize_salary(row.get('è–ªèµ„'))
                if standardized:
                    df_cleaned.at[idx, 'è–ªèµ„'] = standardized
            
            df_cleaned.to_csv(output_path, index=False, encoding='utf-8')
            total_stats['cleaned'] += len(df_cleaned)
        
        print(f"  âœ… åŸå§‹: {total_stats['original']}, æ¸…æ´—å: {total_stats['cleaned']}, å»é‡: {total_stats['duplicates']}")
        return total_stats

# ==================== Neo4jä¸Šä¼ å™¨ ====================
class Neo4jUploader:
    """Neo4jçŸ¥è¯†å›¾è°±ä¸Šä¼ å™¨"""
    
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        print(f"âœ… å·²è¿æ¥åˆ°Neo4j: {uri}")
        
        # æ˜¾ç¤ºå½“å‰æ•°æ®åº“çŠ¶æ€
        try:
            total_nodes, total_rels = get_neo4j_stats(self.driver)
            print(f"ğŸ“Š å½“å‰æ•°æ®åº“: {total_nodes:,} ä¸ªèŠ‚ç‚¹, {total_rels:,} æ¡å…³ç³»")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–æ•°æ®åº“ç»Ÿè®¡: {e}")
    
    def close(self):
        self.driver.close()
    
    def _is_valid_value(self, value):
        """æ£€æŸ¥å€¼æ˜¯å¦æœ‰æ•ˆ"""
        if value is None:
            return False
        if pd.isna(value):
            return False
        if isinstance(value, str) and value.strip() == '':
            return False
        return True
    
    def create_job_graph(self, job_data):
        """åˆ›å»ºèŒä½å›¾è°±"""
        with self.driver.session() as session:
            city = job_data.get('åŸå¸‚')
            company = job_data.get('å…¬å¸')
            industry = job_data.get('è¡Œä¸š')
            scale = job_data.get('å…¬å¸è§„æ¨¡', '')
            
            has_city = self._is_valid_value(city)
            has_company = self._is_valid_value(company)
            has_industry = self._is_valid_value(industry)
            
            # åˆ›å»ºåŸå¸‚
            if has_city:
                session.run("MERGE (city:City {name: $name})", name=city)
            
            # åˆ›å»ºå…¬å¸
            if has_company:
                session.run("""
                    MERGE (c:Company {name: $name})
                    SET c.scale = $scale
                """, name=company, scale=scale if self._is_valid_value(scale) else '')
                
                if has_city:
                    session.run("""
                        MATCH (c:Company {name: $company})
                        MATCH (city:City {name: $city})
                        MERGE (c)-[:LOCATED_IN]->(city)
                    """, company=company, city=city)
            
            # åˆ›å»ºè¡Œä¸š
            if has_industry:
                session.run("MERGE (i:Industry {name: $name})", name=industry)
            
            # åˆ›å»ºèŒä½
            job_url = job_data.get('èŒä½URL', f"job_{hash(str(job_data))}")
            session.run("""
                MERGE (j:Job {url: $url})
                SET j.title = $title,
                    j.experience = $experience,
                    j.education = $education,
                    j.description = $description,
                    j.salary = $salary,
                    j.salary_min = $salary_min,
                    j.salary_max = $salary_max,
                    j.annual_months = $annual_months,
                    j.publish_time = $publish_time
            """,
                url=job_url,
                title=job_data.get('èŒä½', ''),
                experience=job_data.get('ç»éªŒ', ''),
                education=job_data.get('å­¦å†', ''),
                description=str(job_data.get('å·¥ä½œæè¿°', ''))[:500],
                salary=job_data.get('è–ªèµ„', ''),
                salary_min=job_data.get('è–ªèµ„_æœ€å°å€¼'),
                salary_max=job_data.get('è–ªèµ„_æœ€å¤§å€¼'),
                annual_months=job_data.get('å¹´è–ªæœˆæ•°'),
                publish_time=job_data.get('å‘å¸ƒæ—¶é—´', '')
            )
            
            # åˆ›å»ºå…³ç³»
            if has_company:
                session.run("""
                    MATCH (j:Job {url: $url})
                    MATCH (c:Company {name: $company})
                    MERGE (j)-[:OFFERED_BY]->(c)
                """, url=job_url, company=company)
            
            if has_industry:
                session.run("""
                    MATCH (j:Job {url: $url})
                    MATCH (i:Industry {name: $industry})
                    MERGE (j)-[:BELONGS_TO_INDUSTRY]->(i)
                """, url=job_url, industry=industry)
            
            # åˆ›å»ºæŠ€èƒ½å…³ç³»
            skills = job_data.get('æŠ€èƒ½', '')
            if skills and isinstance(skills, str):
                skill_list = re.split(r'[,ï¼Œã€;ï¼›/\|]', skills)
                for skill in skill_list:
                    skill = skill.strip()
                    if skill and len(skill) >= 2:
                        session.run("MERGE (s:Skill {name: $name})", name=skill)
                        session.run("""
                            MATCH (j:Job {url: $url})
                            MATCH (s:Skill {name: $skill})
                            MERGE (j)-[:REQUIRES_SKILL]->(s)
                        """, url=job_url, skill=skill)
    
    def upload_city(self, input_dir):
        """ä¸Šä¼ æŒ‡å®šåŸå¸‚çš„æ•°æ®"""
        csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
        total_count = 0
        
        print(f"ğŸ“ ä¸Šä¼ åˆ°Neo4j: {input_dir}")
        
        for csv_file in csv_files:
            file_path = os.path.join(input_dir, csv_file)
            df = pd.read_csv(file_path, encoding='utf-8')
            
            for _, row in df.iterrows():
                job_data = row.to_dict()
                self.create_job_graph(job_data)
                total_count += 1
                
                if total_count % 100 == 0:
                    print(f"  ğŸ“Š è¿›åº¦: {total_count}")
        
        print(f"  âœ… ä¸Šä¼ å®Œæˆ: {total_count} æ¡è®°å½•")
        return total_count

# ==================== æµæ°´çº¿ä¸»ç¨‹åº ====================
class JobDataPipeline:
    """æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self):
        self.config = CONFIG
        script_dir = os.path.dirname(os.path.abspath(__file__))
        base_data_dir = self.config.get('base_data_dir', '.')
        self.base_dir = os.path.normpath(os.path.join(script_dir, base_data_dir))
        self.first_cleaner = FirstCleaner(self.config['required_fields'])
        self.second_cleaner = SecondCleaner()
    
    def find_cities(self):
        """æŸ¥æ‰¾æ‰€æœ‰åŸå¸‚ç›®å½•"""
        cities = []
        for item in os.listdir(self.base_dir):
            if os.path.isdir(os.path.join(self.base_dir, item)) and item.startswith('data_'):
                city_name = item.replace('data_', '')
                cities.append(city_name)
        return cities
    
    def process_city(self, city_name, clean1=True, clean2=True, upload=True):
        """å¤„ç†å•ä¸ªåŸå¸‚æ•°æ®"""
        print(f"\n{'='*60}")
        print(f"ğŸ™ï¸  å¤„ç†åŸå¸‚: {city_name.upper()}")
        print(f"{'='*60}")
        
        # ç›®å½•è·¯å¾„
        raw_dir = os.path.join(self.base_dir, f'data_{city_name}')
        clean1_dir = os.path.join(self.base_dir, f'ç¬¬ä¸€æ¬¡æ¸…æ´—_{city_name}')
        clean2_dir = os.path.join(self.base_dir, 'ç¬¬äºŒæ¬¡æ¸…æ´—_è¿›é˜¶', f'data_{city_name}_advanced')
        
        if not os.path.exists(raw_dir):
            print(f"âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {raw_dir}")
            return
        
        # ç¬¬ä¸€æ¬¡æ¸…æ´—
        if clean1:
            stats1 = self.first_cleaner.clean_city(raw_dir, clean1_dir)
        else:
            print(f"â­ï¸  è·³è¿‡ç¬¬ä¸€æ¬¡æ¸…æ´—")
        
        # ç¬¬äºŒæ¬¡æ¸…æ´—
        if clean2:
            source_dir = clean1_dir if clean1 else raw_dir
            stats2 = self.second_cleaner.clean_city(source_dir, clean2_dir)
        else:
            print(f"â­ï¸  è·³è¿‡ç¬¬äºŒæ¬¡æ¸…æ´—")
        
        # ä¸Šä¼ Neo4j
        if upload:
            # å®‰å…¨ç¡®è®¤
            if not confirm_neo4j_operation():
                print(f"â­ï¸  è·³è¿‡ä¸Šä¼ Neo4jï¼ˆç”¨æˆ·å–æ¶ˆï¼‰")
                return
            
            uploader = Neo4jUploader(
                self.config['neo4j']['uri'],
                self.config['neo4j']['user'],
                self.config['neo4j']['password']
            )
            try:
                uploader.upload_city(clean2_dir)
            finally:
                uploader.close()
        else:
            print(f"â­ï¸  è·³è¿‡ä¸Šä¼ Neo4j")
        
        print(f"âœ… åŸå¸‚ {city_name} å¤„ç†å®Œæˆ")

def main():
    parser = argparse.ArgumentParser(description='èŒä¸šæ•°æ®å¤„ç†æµæ°´çº¿')
    parser.add_argument('--city', type=str, help='æŒ‡å®šå¤„ç†çš„åŸå¸‚ï¼ˆå¦‚shanghaiï¼‰')
    parser.add_argument('--all-new', action='store_true', help='å¤„ç†æ‰€æœ‰æ–°åŸå¸‚')
    parser.add_argument('--clean1', action='store_true', help='æ‰§è¡Œç¬¬ä¸€æ¬¡æ¸…æ´—')
    parser.add_argument('--clean2', action='store_true', help='æ‰§è¡Œç¬¬äºŒæ¬¡æ¸…æ´—')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ°Neo4j')
    parser.add_argument('--all', action='store_true', help='æ‰§è¡Œå®Œæ•´æµç¨‹')
    
    args = parser.parse_args()
    
    pipeline = JobDataPipeline()
    
    # ç¡®å®šå¤„ç†é˜¶æ®µ
    if args.all:
        clean1, clean2, upload = True, True, True
    else:
        clean1 = args.clean1
        clean2 = args.clean2
        upload = args.upload
        if not (clean1 or clean2 or upload):
            clean1, clean2, upload = True, True, True
    
    print("="*60)
    print("ğŸ—ï¸  èŒä¸šæ•°æ®å¤„ç†æµæ°´çº¿")  
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    # å¤„ç†åŸå¸‚
    if args.city:
        pipeline.process_city(args.city, clean1, clean2, upload)
    elif args.all_new:
        cities = pipeline.find_cities()
        processed = pipeline.config.get('processed_cities', [])
        new_cities = [c for c in cities if c not in processed]
        
        if new_cities:
            print(f"\nå‘ç° {len(new_cities)} ä¸ªæ–°åŸå¸‚: {', '.join(new_cities)}")
            for city in new_cities:
                pipeline.process_city(city, clean1, clean2, upload)
        else:
            print("\næ²¡æœ‰å‘ç°æ–°åŸå¸‚")
    else:
        print("\nâŒ è¯·æŒ‡å®š --city æˆ– --all-new å‚æ•°")
        print("ç¤ºä¾‹: python job_data_pipeline.py --city shanghai --all")
    
    print("\n" + "="*60)
    print("âœ… æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
    print("="*60)

if __name__ == '__main__':
    main()
