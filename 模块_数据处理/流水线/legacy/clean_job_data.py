#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èŒä½æ•°æ®æ¸…æ´—è„šæœ¬
================
åŠŸèƒ½ï¼š
1. æ£€æŸ¥å¹¶åˆ é™¤ç¼ºå°‘"èŒä½/å…¬å¸/å·¥ä½œæè¿°"çš„è®°å½•
2. ç”Ÿæˆè¯¦ç»†çš„æ¸…æ´—æ—¥å¿—ï¼ˆåŒ…å«åˆ é™¤ç†ç”±ï¼‰
3. æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªåŸå¸‚ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
4. æ¯ä¸ªåŸå¸‚å•ç‹¬è¾“å‡ºæ¸…æ´—ç»“æœå’Œæ—¥å¿—

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2026-01-12
"""

import os
import pandas as pd
from datetime import datetime
import logging

# ==================== é…ç½® ====================
# å¿…é¡»å­—æ®µ - ç¼ºå°‘ä»»æ„ä¸€ä¸ªåˆ™åˆ é™¤è¯¥è®°å½•
REQUIRED_FIELDS = ['èŒä½', 'å…¬å¸', 'å·¥ä½œæè¿°']

# éœ€è¦å¤„ç†çš„åŸå¸‚æ•°æ®ç›®å½•
DATA_DIRS = [
    'data_beijing',
    'data_chengdu', 
    'data_chongqing',
    'data_hangzhou',
    'data_nanjin',
    'data_shenzhen',
    'data_wuhan',
    'data_xiamen',
    'data_zhengzhou'
]

# ==================== æ—¥å¿—é…ç½® ====================
def setup_logger(log_file, city_name):
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(f'DataCleaner_{city_name}')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ä¹‹å‰çš„å¤„ç†å™¨
    logger.handlers = []
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ==================== æ ¸å¿ƒæ¸…æ´—å‡½æ•° ====================
def check_required_fields(row, required_fields):
    """
    æ£€æŸ¥è®°å½•æ˜¯å¦ç¼ºå°‘å¿…é¡»å­—æ®µ
    
    è¿”å›:
        (bool, str): (æ˜¯å¦æœ‰æ•ˆ, åˆ é™¤ç†ç”±)
    """
    missing_fields = []
    
    for field in required_fields:
        value = row.get(field, None)
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå€¼
        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):
            missing_fields.append(field)
    
    if missing_fields:
        return False, f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {', '.join(missing_fields)}"
    
    return True, ""


def clean_single_file(input_path, output_path, logger):
    """
    æ¸…æ´—å•ä¸ªCSVæ–‡ä»¶
    """
    filename = os.path.basename(input_path)
    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")
    logger.info(f"{'='*60}")
    
    stats = {
        'filename': filename,
        'original_count': 0,
        'cleaned_count': 0,
        'removed_count': 0,
        'removal_reasons': []
    }
    
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(input_path, encoding='utf-8')
        stats['original_count'] = len(df)
        logger.info(f"åŸå§‹è®°å½•æ•°: {len(df)}")
        
        # æ£€æŸ¥å¿…é¡»å­—æ®µæ˜¯å¦å­˜åœ¨
        for field in REQUIRED_FIELDS:
            if field not in df.columns:
                logger.error(f"é”™è¯¯: æ–‡ä»¶ç¼ºå°‘å¿…é¡»åˆ— '{field}'")
                return stats
        
        # å­˜å‚¨æœ‰æ•ˆè®°å½•å’Œåˆ é™¤è®°å½•
        valid_rows = []
        removed_entries = []
        
        for idx, row in df.iterrows():
            is_valid, reason = check_required_fields(row, REQUIRED_FIELDS)
            
            if is_valid:
                valid_rows.append(row)
            else:
                job_title = row.get('èŒä½', 'N/A')
                company = row.get('å…¬å¸', 'N/A')
                removed_entry = {
                    'row_number': idx + 2,
                    'job_title': job_title if pd.notna(job_title) else 'N/A',
                    'company': company if pd.notna(company) else 'N/A',
                    'reason': reason
                }
                removed_entries.append(removed_entry)
                logger.warning(f"åˆ é™¤è®°å½• [è¡Œ{idx+2}]: èŒä½='{removed_entry['job_title']}', "
                             f"å…¬å¸='{removed_entry['company']}' | ç†ç”±: {reason}")
        
        stats['cleaned_count'] = len(valid_rows)
        stats['removed_count'] = len(removed_entries)
        stats['removal_reasons'] = removed_entries
        
        if valid_rows:
            cleaned_df = pd.DataFrame(valid_rows)
            cleaned_df.to_csv(output_path, index=False, encoding='utf-8')
            logger.info(f"æ¸…æ´—åè®°å½•æ•°: {len(valid_rows)}")
            logger.info(f"åˆ é™¤è®°å½•æ•°: {len(removed_entries)}")
            logger.info(f"ä¿ç•™ç‡: {len(valid_rows)/len(df)*100:.2f}%")
        else:
            logger.warning(f"è­¦å‘Š: æ¸…æ´—åæ²¡æœ‰å‰©ä½™æœ‰æ•ˆè®°å½•!")
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise
    
    return stats


def generate_city_report(city_name, all_stats, output_path, logger):
    """ç”ŸæˆåŸå¸‚æ±‡æ€»æŠ¥å‘Š"""
    total_original = sum(s['original_count'] for s in all_stats)
    total_cleaned = sum(s['cleaned_count'] for s in all_stats)
    total_removed = sum(s['removed_count'] for s in all_stats)
    
    report_lines = [
        "="*70,
        f"ã€{city_name}ã€‘èŒä½æ•°æ®æ¸…æ´—æŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "="*70,
        "",
        "ã€ç»Ÿè®¡æ‘˜è¦ã€‘",
        f"  å¤„ç†æ–‡ä»¶æ•°: {len(all_stats)}",
        f"  åŸå§‹æ€»è®°å½•æ•°: {total_original}",
        f"  æ¸…æ´—åæ€»è®°å½•æ•°: {total_cleaned}",
        f"  åˆ é™¤æ€»è®°å½•æ•°: {total_removed}",
        f"  ä¿ç•™ç‡: {total_cleaned/total_original*100:.2f}%" if total_original > 0 else "  ä¿ç•™ç‡: N/A",
        "",
        "-"*70,
        "ã€åˆ é™¤è®°å½•æ˜ç»†ã€‘",
        "-"*70,
    ]
    
    for stats in all_stats:
        if stats['removal_reasons']:
            report_lines.append(f"\næ–‡ä»¶: {stats['filename']}")
            for entry in stats['removal_reasons']:
                report_lines.append(
                    f"  è¡Œ{entry['row_number']}: [{entry['job_title']}]@[{entry['company']}] | {entry['reason']}"
                )
    
    # å¦‚æœæ²¡æœ‰åˆ é™¤è®°å½•
    if total_removed == 0:
        report_lines.append("\næ— åˆ é™¤è®°å½•ï¼Œæ‰€æœ‰æ•°æ®å®Œæ•´æœ‰æ•ˆã€‚")
    
    report_lines.extend(["", "="*70, "æŠ¥å‘Šç»“æŸ", "="*70])
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(report_lines))
    
    logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    return {
        'city': city_name,
        'files': len(all_stats),
        'original': total_original,
        'cleaned': total_cleaned,
        'removed': total_removed
    }


def process_city(base_dir, city_dir, timestamp):
    """å¤„ç†å•ä¸ªåŸå¸‚çš„æ•°æ®"""
    city_name = city_dir.replace('data_', '').upper()
    input_dir = os.path.join(base_dir, city_dir)
    
    # æ¯ä¸ªåŸå¸‚å•ç‹¬çš„è¾“å‡ºç›®å½•
    output_dir = os.path.join(base_dir, f'{city_dir}_cleaned')
    log_dir = os.path.join(output_dir, 'logs')
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(log_dir, f'cleaning_log_{timestamp}.log')
    logger = setup_logger(log_file, city_name)
    
    logger.info("="*60)
    logger.info(f"å¼€å§‹å¤„ç†åŸå¸‚: {city_name}")
    logger.info(f"è¾“å…¥ç›®å½•: {input_dir}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info("="*60)
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
    
    if not csv_files:
        logger.warning(f"åŸå¸‚ {city_name} æœªæ‰¾åˆ°CSVæ–‡ä»¶!")
        return None
    
    logger.info(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    all_stats = []
    
    for csv_file in sorted(csv_files):
        input_path = os.path.join(input_dir, csv_file)
        output_filename = csv_file.replace('.csv', '_cleaned.csv')
        output_path = os.path.join(output_dir, output_filename)
        
        try:
            stats = clean_single_file(input_path, output_path, logger)
            all_stats.append(stats)
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {csv_file} å¤±è´¥: {str(e)}")
            continue
    
    # ç”ŸæˆåŸå¸‚æŠ¥å‘Š
    report_path = os.path.join(log_dir, f'cleaning_report_{timestamp}.txt')
    city_summary = generate_city_report(city_name, all_stats, report_path, logger)
    
    logger.info(f"\n{city_name} å¤„ç†å®Œæˆ!")
    
    # å…³é—­æ—¥å¿—å¤„ç†å™¨
    for handler in logger.handlers[:]:
        handler.close()
        logger.removeHandler(handler)
    
    return city_summary


def main():
    """ä¸»å‡½æ•°"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print("\n" + "="*60)
    print("ğŸš€ èŒä½æ•°æ®æ¸…æ´—ç¨‹åºå¯åŠ¨")
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ åŸºç¡€ç›®å½•: {base_dir}")
    print(f"ğŸ™ï¸  å¾…å¤„ç†åŸå¸‚: {len(DATA_DIRS)} ä¸ª")
    print("="*60 + "\n")
    
    all_city_summaries = []
    
    for city_dir in DATA_DIRS:
        city_path = os.path.join(base_dir, city_dir)
        if os.path.exists(city_path):
            summary = process_city(base_dir, city_dir, timestamp)
            if summary:
                all_city_summaries.append(summary)
        else:
            print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {city_dir}")
    
    # æ‰“å°æ€»æ±‡æ€»
    print("\n" + "="*60)
    print("ğŸ“Š å…¨éƒ¨åŸå¸‚æ¸…æ´—å®Œæˆ - æ€»ä½“ç»Ÿè®¡")
    print("="*60)
    
    total_files = sum(s['files'] for s in all_city_summaries)
    total_original = sum(s['original'] for s in all_city_summaries)
    total_cleaned = sum(s['cleaned'] for s in all_city_summaries)
    total_removed = sum(s['removed'] for s in all_city_summaries)
    
    print(f"{'åŸå¸‚':<12} {'æ–‡ä»¶æ•°':<8} {'åŸå§‹è®°å½•':<12} {'ä¿ç•™è®°å½•':<12} {'åˆ é™¤è®°å½•':<10} {'ä¿ç•™ç‡':<10}")
    print("-"*60)
    
    for s in all_city_summaries:
        rate = f"{s['cleaned']/s['original']*100:.1f}%" if s['original'] > 0 else "N/A"
        print(f"{s['city']:<12} {s['files']:<8} {s['original']:<12} {s['cleaned']:<12} {s['removed']:<10} {rate:<10}")
    
    print("-"*60)
    total_rate = f"{total_cleaned/total_original*100:.1f}%" if total_original > 0 else "N/A"
    print(f"{'æ€»è®¡':<12} {total_files:<8} {total_original:<12} {total_cleaned:<12} {total_removed:<10} {total_rate:<10}")
    print("="*60)
    
    print("\nâœ… æ¸…æ´—åçš„æ–‡ä»¶ä¿å­˜åœ¨å„åŸå¸‚ç›®å½•ä¸‹çš„ *_cleaned æ–‡ä»¶å¤¹ä¸­")
    print("ğŸ“‹ è¯¦ç»†æ—¥å¿—å’ŒæŠ¥å‘Šä¿å­˜åœ¨å„æ–‡ä»¶å¤¹çš„ logs å­ç›®å½•ä¸­\n")


if __name__ == '__main__':
    main()
