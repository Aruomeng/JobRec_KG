#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èŒä½æ•°æ®è¿›é˜¶æ¸…æ´—è„šæœ¬
====================
åŠŸèƒ½ï¼š
1. å»é‡ - æ£€æµ‹å¹¶åˆ é™¤é‡å¤èŒä½ï¼ˆèŒä½+å…¬å¸+å·¥ä½œæè¿°å®Œå…¨é‡å¤ï¼‰
2. è–ªèµ„æ ‡å‡†åŒ– - å°†"é¢è®®"è½¬ä¸ºnullï¼Œç»Ÿä¸€è–ªèµ„æ ¼å¼
3. æ–‡æœ¬æ¸…ç† - å»é™¤å·¥ä½œæè¿°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
4. æ•°æ®å¢å¼º - æå–è–ªèµ„èŒƒå›´çš„æœ€å°å€¼/æœ€å¤§å€¼

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2026-01-12
"""

import os
import re
import pandas as pd
from datetime import datetime
import logging

# ==================== é…ç½® ====================
# è¾“å…¥ç›®å½•ï¼ˆç¬¬ä¸€æ¬¡æ¸…æ´—åçš„æ•°æ®ï¼‰
INPUT_BASE_DIR = 'ç¬¬ä¸€æ¬¡æ¸…æ´—'

# è¾“å‡ºç›®å½•
OUTPUT_BASE_DIR = 'ç¬¬äºŒæ¬¡æ¸…æ´—_è¿›é˜¶'

# åŸå¸‚ç›®å½•åˆ—è¡¨
CITY_DIRS = [
    'data_beijing_cleaned',
    'data_chengdu_cleaned',
    'data_chongqing_cleaned',
    'data_hangzhou_cleaned',
    'data_nanjin_cleaned',
    'data_shenzhen_cleaned',
    'data_wuhan_cleaned',
    'data_xiamen_cleaned',
    'data_zhengzhou_cleaned'
]

# ç”¨äºå»é‡åˆ¤æ–­çš„å­—æ®µ
DEDUP_FIELDS = ['èŒä½', 'å…¬å¸', 'å·¥ä½œæè¿°']

# ==================== æ—¥å¿—é…ç½® ====================
def setup_logger(log_file, city_name):
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(f'AdvancedCleaner_{city_name}')
    logger.setLevel(logging.INFO)
    logger.handlers = []
    
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ==================== è–ªèµ„å¤„ç†å‡½æ•° ====================
def parse_salary(salary_str):
    """
    è§£æè–ªèµ„å­—ç¬¦ä¸²ï¼Œæå–æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼ˆå•ä½ï¼šå…ƒ/æœˆï¼‰
    
    æ”¯æŒæ ¼å¼:
    - "4-7ä¸‡" -> (40000, 70000)
    - "3-5ä¸‡Â·15è–ª" -> (30000, 50000)  # æœˆè–ªåŸºç¡€å€¼
    - "2.5-4.5ä¸‡Â·14è–ª" -> (25000, 45000)
    - "1.5-3ä¸‡" -> (15000, 30000)
    - "é¢è®®" -> (None, None)
    - "20-99äºº" -> è¿™ä¸æ˜¯è–ªèµ„ï¼Œè¿”å›None
    
    è¿”å›:
        (min_salary, max_salary, annual_months): æœ€å°æœˆè–ªã€æœ€å¤§æœˆè–ªã€å¹´è–ªæœˆæ•°
    """
    if pd.isna(salary_str) or not isinstance(salary_str, str):
        return None, None, None
    
    salary_str = salary_str.strip()
    
    # é¢è®®æƒ…å†µ
    if salary_str == 'é¢è®®' or salary_str == '':
        return None, None, None
    
    # æå–å¹´è–ªæœˆæ•° (å¦‚ 15è–ª, 14è–ª, 16è–ª)
    annual_months = 12  # é»˜è®¤12ä¸ªæœˆ
    months_match = re.search(r'(\d+)è–ª', salary_str)
    if months_match:
        annual_months = int(months_match.group(1))
    
    # åŒ¹é…è–ªèµ„èŒƒå›´: æ•°å­—-æ•°å­—ä¸‡ æˆ– æ•°å­—-æ•°å­—åƒ
    # æ ¼å¼1: X-Yä¸‡ (å¦‚ 4-7ä¸‡, 2.5-4.5ä¸‡)
    pattern_wan = r'([\d.]+)[-~]([\d.]+)\s*ä¸‡'
    match_wan = re.search(pattern_wan, salary_str)
    
    if match_wan:
        try:
            min_val = float(match_wan.group(1)) * 10000
            max_val = float(match_wan.group(2)) * 10000
            return int(min_val), int(max_val), annual_months
        except ValueError:
            pass
    
    # æ ¼å¼2: X-Yk (å¦‚ 15-30k)
    pattern_k = r'([\d.]+)[-~]([\d.]+)\s*[kK]'
    match_k = re.search(pattern_k, salary_str)
    
    if match_k:
        try:
            min_val = float(match_k.group(1)) * 1000
            max_val = float(match_k.group(2)) * 1000
            return int(min_val), int(max_val), annual_months
        except ValueError:
            pass
    
    # æ— æ³•è§£æ
    return None, None, None


def standardize_salary(salary_str):
    """
    æ ‡å‡†åŒ–è–ªèµ„å­—ç¬¦ä¸²ä¸ºæ•°å€¼æ ¼å¼
    - "é¢è®®" -> None
    - "4-7ä¸‡" -> "40000-70000"
    - "4-7ä¸‡Â·15è–ª" -> "40000-70000"
    - "2.5-4.5ä¸‡" -> "25000-45000"
    - "15-30k" -> "15000-30000"
    """
    if pd.isna(salary_str) or not isinstance(salary_str, str):
        return None
    
    salary_str = salary_str.strip()
    
    if salary_str == 'é¢è®®' or salary_str == '':
        return None
    
    # æ ¼å¼1: X-Yä¸‡ (å¦‚ 4-7ä¸‡, 2.5-4.5ä¸‡, 4-7ä¸‡Â·15è–ª)
    pattern_wan = r'([\d.]+)[-~]([\d.]+)\s*ä¸‡'
    match_wan = re.search(pattern_wan, salary_str)
    
    if match_wan:
        try:
            min_val = int(float(match_wan.group(1)) * 10000)
            max_val = int(float(match_wan.group(2)) * 10000)
            return f"{min_val}-{max_val}"
        except ValueError:
            pass
    
    # æ ¼å¼2: X-Yk (å¦‚ 15-30k)
    pattern_k = r'([\d.]+)[-~]([\d.]+)\s*[kK]'
    match_k = re.search(pattern_k, salary_str)
    
    if match_k:
        try:
            min_val = int(float(match_k.group(1)) * 1000)
            max_val = int(float(match_k.group(2)) * 1000)
            return f"{min_val}-{max_val}"
        except ValueError:
            pass
    
    # æ— æ³•è§£æçš„æ ¼å¼ï¼Œè¿”å›None
    return None


# ==================== æ–‡æœ¬æ¸…ç†å‡½æ•° ====================
def clean_text(text):
    """
    æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
    
    å¤„ç†ï¼š
    - å»é™¤ä¸å¯è§å­—ç¬¦
    - å»é™¤å¤šä½™ç©ºç™½
    - å»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    """
    if pd.isna(text) or not isinstance(text, str):
        return text
    
    # å»é™¤å¸¸è§ç‰¹æ®Šå­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹
    cleaned = text
    
    # æ›¿æ¢ç‰¹æ®Šç©ºç™½å­—ç¬¦ä¸ºæ™®é€šç©ºæ ¼
    cleaned = re.sub(r'[\u00a0\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a\u200b\u3000]', ' ', cleaned)
    
    # å»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦ï¼‰
    cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', cleaned)
    
    # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    cleaned = re.sub(r' +', ' ', cleaned)
    
    # å»é™¤é¦–å°¾ç©ºç™½
    cleaned = cleaned.strip()
    
    return cleaned

# ==================== æ ¸å¿ƒæ¸…æ´—å‡½æ•° ====================
def advanced_clean_dataframe(df, logger):
    """
    è¿›é˜¶æ¸…æ´—DataFrame
    
    è¿”å›:
        (cleaned_df, stats): æ¸…æ´—åçš„DataFrameå’Œç»Ÿè®¡ä¿¡æ¯
    """
    stats = {
        'original_count': len(df),
        'duplicates_removed': 0,
        'salary_standardized': 0,
        'salary_negotiable': 0,  # é¢è®®çš„æ•°é‡
        'text_cleaned': 0,
        'salary_parsed': 0,
        'final_count': 0
    }
    
    logger.info(f"å¼€å§‹è¿›é˜¶æ¸…æ´—ï¼ŒåŸå§‹è®°å½•æ•°: {len(df)}")
    
    # ===== 1. å»é‡ =====
    logger.info("æ­¥éª¤1: æ£€æµ‹é‡å¤è®°å½•...")
    
    # åˆ›å»ºå»é‡æ ‡è¯†åˆ—
    df['_dedup_key'] = df[DEDUP_FIELDS].apply(
        lambda x: '|||'.join(str(v).strip() for v in x), axis=1
    )
    
    # æ ‡è®°é‡å¤è¡Œï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
    duplicates_mask = df.duplicated(subset=['_dedup_key'], keep='first')
    duplicate_count = duplicates_mask.sum()
    stats['duplicates_removed'] = duplicate_count
    
    if duplicate_count > 0:
        # è®°å½•è¢«åˆ é™¤çš„é‡å¤è®°å½•
        duplicate_rows = df[duplicates_mask]
        for idx, row in duplicate_rows.head(10).iterrows():  # åªè®°å½•å‰10æ¡
            logger.warning(f"åˆ é™¤é‡å¤: [{row['èŒä½']}]@[{row['å…¬å¸']}]")
        if duplicate_count > 10:
            logger.warning(f"... è¿˜æœ‰ {duplicate_count - 10} æ¡é‡å¤è®°å½•è¢«åˆ é™¤")
        
        df = df[~duplicates_mask].copy()
        logger.info(f"å»é‡å®Œæˆï¼Œåˆ é™¤ {duplicate_count} æ¡é‡å¤è®°å½•")
    else:
        logger.info("æœªå‘ç°é‡å¤è®°å½•")
    
    # åˆ é™¤ä¸´æ—¶åˆ—
    df = df.drop(columns=['_dedup_key'])
    
    # ===== 2. è–ªèµ„å¤„ç†ï¼ˆå…ˆè§£æåŸå§‹å€¼ï¼Œå†æ ‡å‡†åŒ–ï¼‰ =====
    logger.info("æ­¥éª¤2: è–ªèµ„å¤„ç†...")
    
    if 'è–ªèµ„' in df.columns:
        # ä¿å­˜åŸå§‹è–ªèµ„ç”¨äºè§£æ
        original_salary = df['è–ªèµ„'].copy()
        
        # ç»Ÿè®¡é¢è®®æ•°é‡
        negotiable_mask = original_salary.apply(
            lambda x: x == 'é¢è®®' if isinstance(x, str) else False
        )
        stats['salary_negotiable'] = negotiable_mask.sum()
        
        # å…ˆä»åŸå§‹è–ªèµ„è§£ææ•°å€¼ï¼ˆæœ€å°å€¼/æœ€å¤§å€¼/å¹´è–ªæœˆæ•°ï¼‰
        salary_info = original_salary.apply(parse_salary)
        
        df['è–ªèµ„_æœ€å°å€¼'] = salary_info.apply(lambda x: x[0] if x else None)
        df['è–ªèµ„_æœ€å¤§å€¼'] = salary_info.apply(lambda x: x[1] if x else None)
        df['å¹´è–ªæœˆæ•°'] = salary_info.apply(lambda x: x[2] if x else None)
        
        # ç»Ÿè®¡æˆåŠŸè§£æçš„æ•°é‡
        parsed_count = df['è–ªèµ„_æœ€å°å€¼'].notna().sum()
        stats['salary_parsed'] = parsed_count
        
        # å†æ ‡å‡†åŒ–è–ªèµ„åˆ—ï¼ˆå°†"4-7ä¸‡"è½¬ä¸º"40000-70000"ï¼Œ"é¢è®®"è½¬ä¸ºNoneï¼‰
        df['è–ªèµ„'] = original_salary.apply(standardize_salary)
        stats['salary_standardized'] = len(df)
        
        logger.info(f"è–ªèµ„å¤„ç†å®Œæˆï¼šè§£ææˆåŠŸ {parsed_count} æ¡ï¼Œ'é¢è®®' {stats['salary_negotiable']} æ¡")
    
    # ===== 3. æ–‡æœ¬æ¸…ç† =====
    logger.info("æ­¥éª¤3: æ¸…ç†æ–‡æœ¬ç‰¹æ®Šå­—ç¬¦...")
    
    text_columns = ['èŒä½', 'å…¬å¸', 'å·¥ä½œæè¿°', 'æŠ€èƒ½']
    for col in text_columns:
        if col in df.columns:
            df[col] = df[col].apply(clean_text)
    
    stats['text_cleaned'] = len(df)
    logger.info("æ–‡æœ¬æ¸…ç†å®Œæˆ")
    
    stats['final_count'] = len(df)
    
    return df, stats


def process_city(base_dir, input_city_dir, output_city_dir, timestamp, logger):
    """å¤„ç†å•ä¸ªåŸå¸‚çš„æ•°æ®"""
    
    city_name = input_city_dir.replace('data_', '').replace('_cleaned', '').upper()
    
    input_dir = os.path.join(base_dir, INPUT_BASE_DIR, input_city_dir)
    output_dir = os.path.join(base_dir, OUTPUT_BASE_DIR, output_city_dir)
    
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info("="*60)
    logger.info(f"å¤„ç†åŸå¸‚: {city_name}")
    logger.info("="*60)
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶ï¼ˆæ’é™¤logsç›®å½•ï¼‰
    csv_files = [f for f in os.listdir(input_dir) 
                 if f.endswith('.csv') and not f.startswith('.')]
    
    if not csv_files:
        logger.warning(f"åŸå¸‚ {city_name} æœªæ‰¾åˆ°CSVæ–‡ä»¶!")
        return None
    
    logger.info(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    city_stats = {
        'city': city_name,
        'files': len(csv_files),
        'original': 0,
        'final': 0,
        'duplicates': 0,
        'negotiable': 0,
        'salary_parsed': 0
    }
    
    for csv_file in sorted(csv_files):
        input_path = os.path.join(input_dir, csv_file)
        output_filename = csv_file.replace('_cleaned.csv', '_advanced.csv')
        output_path = os.path.join(output_dir, output_filename)
        
        logger.info(f"\nå¤„ç†æ–‡ä»¶: {csv_file}")
        
        try:
            df = pd.read_csv(input_path, encoding='utf-8')
            cleaned_df, stats = advanced_clean_dataframe(df, logger)
            
            # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
            cleaned_df.to_csv(output_path, index=False, encoding='utf-8')
            
            # ç´¯è®¡ç»Ÿè®¡
            city_stats['original'] += stats['original_count']
            city_stats['final'] += stats['final_count']
            city_stats['duplicates'] += stats['duplicates_removed']
            city_stats['negotiable'] += stats['salary_negotiable']
            city_stats['salary_parsed'] += stats['salary_parsed']
            
            logger.info(f"ä¿å­˜è‡³: {output_filename}")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {csv_file} å¤±è´¥: {str(e)}")
            continue
    
    return city_stats


def main():
    """ä¸»å‡½æ•°"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_base = os.path.join(base_dir, OUTPUT_BASE_DIR)
    log_dir = os.path.join(output_base, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    # è®¾ç½®å…¨å±€æ—¥å¿—
    log_file = os.path.join(log_dir, f'advanced_cleaning_{timestamp}.log')
    logger = setup_logger(log_file, 'GLOBAL')
    
    print("\n" + "="*60)
    print("ğŸš€ èŒä½æ•°æ®è¿›é˜¶æ¸…æ´—ç¨‹åºå¯åŠ¨")
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    print("\nğŸ”§ æ¸…æ´—å†…å®¹:")
    print("   1. å»é‡ - åˆ é™¤(èŒä½+å…¬å¸+å·¥ä½œæè¿°)å®Œå…¨é‡å¤çš„è®°å½•")
    print("   2. è–ªèµ„æ ‡å‡†åŒ– - \"é¢è®®\"è½¬ä¸ºç©ºå€¼")
    print("   3. æ–‡æœ¬æ¸…ç† - å»é™¤ç‰¹æ®Šå­—ç¬¦")
    print("   4. æ•°æ®å¢å¼º - æå–è–ªèµ„æœ€å°å€¼/æœ€å¤§å€¼/å¹´è–ªæœˆæ•°")
    print("="*60 + "\n")
    
    all_city_stats = []
    
    for city_dir in CITY_DIRS:
        input_path = os.path.join(base_dir, INPUT_BASE_DIR, city_dir)
        if os.path.exists(input_path):
            output_city_dir = city_dir.replace('_cleaned', '_advanced')
            stats = process_city(base_dir, city_dir, output_city_dir, timestamp, logger)
            if stats:
                all_city_stats.append(stats)
        else:
            print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {city_dir}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š è¿›é˜¶æ¸…æ´—å®Œæˆ - æ€»ä½“ç»Ÿè®¡")
    print("="*70)
    
    total_original = sum(s['original'] for s in all_city_stats)
    total_final = sum(s['final'] for s in all_city_stats)
    total_duplicates = sum(s['duplicates'] for s in all_city_stats)
    total_negotiable = sum(s['negotiable'] for s in all_city_stats)
    total_parsed = sum(s['salary_parsed'] for s in all_city_stats)
    
    print(f"\n{'åŸå¸‚':<12} {'åŸå§‹è®°å½•':<10} {'å»é‡å':<10} {'åˆ é™¤é‡å¤':<10} {'é¢è®®æ•°':<10} {'è–ªèµ„è§£æ':<10}")
    print("-"*70)
    
    for s in all_city_stats:
        print(f"{s['city']:<12} {s['original']:<10} {s['final']:<10} {s['duplicates']:<10} {s['negotiable']:<10} {s['salary_parsed']:<10}")
    
    print("-"*70)
    print(f"{'æ€»è®¡':<12} {total_original:<10} {total_final:<10} {total_duplicates:<10} {total_negotiable:<10} {total_parsed:<10}")
    print("="*70)
    
    print(f"\nğŸ“ˆ æ¸…æ´—æ•ˆæœ:")
    print(f"   â€¢ åŸå§‹è®°å½•: {total_original}")
    print(f"   â€¢ åˆ é™¤é‡å¤: {total_duplicates}")
    print(f"   â€¢ æœ€ç»ˆä¿ç•™: {total_final}")
    print(f"   â€¢ ä¿ç•™ç‡: {total_final/total_original*100:.2f}%")
    print(f"   â€¢ é¢è®®èŒä½: {total_negotiable} ({total_negotiable/total_original*100:.1f}%)")
    print(f"   â€¢ æˆåŠŸè§£æè–ªèµ„: {total_parsed} ({total_parsed/total_original*100:.1f}%)")
    
    print(f"\nâœ… æ¸…æ´—åçš„æ–‡ä»¶ä¿å­˜åœ¨: {output_base}")
    print(f"ğŸ“‹ è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨: {log_file}\n")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    report_path = os.path.join(log_dir, f'advanced_report_{timestamp}.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("èŒä½æ•°æ®è¿›é˜¶æ¸…æ´—æŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*70 + "\n\n")
        f.write("æ¸…æ´—å†…å®¹:\n")
        f.write("1. å»é‡ - åˆ é™¤(èŒä½+å…¬å¸+å·¥ä½œæè¿°)å®Œå…¨é‡å¤çš„è®°å½•\n")
        f.write("2. è–ªèµ„æ ‡å‡†åŒ– - \"é¢è®®\"è½¬ä¸ºç©ºå€¼\n")
        f.write("3. æ–‡æœ¬æ¸…ç† - å»é™¤ç‰¹æ®Šå­—ç¬¦\n")
        f.write("4. æ•°æ®å¢å¼º - æ–°å¢åˆ—: è–ªèµ„_æœ€å°å€¼, è–ªèµ„_æœ€å¤§å€¼, å¹´è–ªæœˆæ•°\n\n")
        f.write("-"*70 + "\n")
        f.write(f"{'åŸå¸‚':<12} {'åŸå§‹è®°å½•':<10} {'å»é‡å':<10} {'åˆ é™¤é‡å¤':<10} {'é¢è®®æ•°':<10} {'è–ªèµ„è§£æ':<10}\n")
        f.write("-"*70 + "\n")
        for s in all_city_stats:
            f.write(f"{s['city']:<12} {s['original']:<10} {s['final']:<10} {s['duplicates']:<10} {s['negotiable']:<10} {s['salary_parsed']:<10}\n")
        f.write("-"*70 + "\n")
        f.write(f"{'æ€»è®¡':<12} {total_original:<10} {total_final:<10} {total_duplicates:<10} {total_negotiable:<10} {total_parsed:<10}\n")
        f.write("="*70 + "\n")
    
    logger.info("è¿›é˜¶æ¸…æ´—å…¨éƒ¨å®Œæˆ!")
    
    # å…³é—­æ—¥å¿—å¤„ç†å™¨
    for handler in logger.handlers[:]:
        handler.close()
        logger.removeHandler(handler)


if __name__ == '__main__':
    main()
